# -*- coding: utf-8 -*-
"""trail.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S5v1Nl1WYBeguqMJb_cuqqd8gvnT2GVU
"""

import torch
import transformers
from transformers import T5Tokenizer, T5ForConditionalGeneration


from transformers import LogitsProcessor,LogitsProcessorList
class ClassifierGuidance(LogitsProcessor):
    def __init__(self, classifier_model, prediction_topk, condition_lambda, condition_class):
        self.classifier_model = classifier_model
        self.prediction_topk = prediction_topk
        self.condition_lambda = condition_lambda
        self.condition_class = condition_class
        self.softmax = torch.nn.Softmax(dim=-1)


    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):
        # input_ids : batch x seq
        # scores : batch x vocab
        batch_size = input_ids.shape[0]
        top_logits, top_indices = scores.topk(self.prediction_topk, dim=1) # batch x topk
        scores_topk_only = torch.full_like(scores, -float('inf')) # batch x vocab
        for i in range(batch_size):
            scores_topk_only[i][top_indices] = top_logits[i].float()
        # prob_topk_only = self.softmax(scores_topk_only) # batch x vocab

        if self.condition_lambda == 0:
            condition_logits_full = torch.zeros_like(scores_topk_only).float()
        else:
            # tplus1_candidates = torch.cat([input_ids.unsqueeze(1).expand(-1, self.prediction_topk, -1), top_indices.unsqueeze(2)], dim=2)[:, :, 1:] # batch x topk x seq+1, with pad dropped
            tplus1_candidates = torch.cat([input_ids.unsqueeze(1).expand(-1, self.prediction_topk, -1), top_indices.unsqueeze(2)], dim=2)[:, :, :] # batch x topk x seq+1
            # classifier_prob : batch*topk x n_class
            classifier_prob = self.classifier_model.predict_proba(
                                                tplus1_candidates.flatten(0, 1).cpu().tolist(), # batch*topk x seq+1
                                                )
            classifier_prob = classifier_prob[:, self.condition_class]
            classifier_prob = classifier_prob.view(batch_size, self.prediction_topk) # batch x topk
            condition_logits = torch.log(classifier_prob) # batch x topk
            condition_logits_full = torch.full_like(scores, -float('inf')) # batch x vocab
            for i in range(batch_size):
                condition_logits_full[i][top_indices] = condition_logits[i].float()

        scores_topk_only = scores_topk_only + self.condition_lambda * condition_logits_full
        return scores_topk_only

class Generate_Sentences:

    def __init__(self, sentence, lmbda) -> None:

        self.sentence = sentence
        self.lmbda = lmbda
        self.formal_sentences = None
        self.informal_sentences = None
        self.paraphrase_texts = None


    def generate_paraphrase_sentences(self,model,tokenizer,no_repeat_ngram_size):

      generated_paraphrase_texts = []
      samples = len(self.sentence)
      data = self.sentence
      text =  ["paraphrase: " + s + "</s>" for s in data]

      for idx in range(samples):
        input_ids = tokenizer(text[idx], return_tensors="pt", padding=True, truncation=True).input_ids
        # Paraphrase model
        result = model.generate(
            input_ids,
            max_new_tokens=100,
            do_sample=False,
            return_dict_in_generate=True,
            output_hidden_states=True,
            output_scores=True,
            no_repeat_ngram_size = 2
        )
        # Decoding the generated sequences for the entire batch
        generated_paraphrase_texts.append(tokenizer.batch_decode(result['sequences'], skip_special_tokens=True)[0])
        self.paraphrase_texts = generated_paraphrase_texts

    def generate_formal_sentences(self, model,tokenizer, classifier_model,no_repeat_ngram_size)-> None:

        generated_sentences  = []

        samples = len(self.sentence)
        data = self.sentence
        text =  ["paraphrase: " + s + "</s>" for s in data]

        logits_processor_list = LogitsProcessorList([
            ClassifierGuidance(classifier_model, 100, condition_lambda=self.lmbda, condition_class=0),
        ])


        for idx in range(samples):
            input_ids = tokenizer(text[idx], return_tensors="pt", padding=True, truncation=True).input_ids
            result = model.generate(
                    input_ids,
                    max_new_tokens=100,
                    do_sample=False,
                    return_dict_in_generate=True,
                    output_hidden_states=True,
                    output_scores=True,
                    logits_processor=logits_processor_list,
                    no_repeat_ngram_size = no_repeat_ngram_size
                )

            generated_sentences.append(tokenizer.batch_decode(result['sequences'], skip_special_tokens=True)[0])

        self.formal_sentences = generated_sentences
        return


    def generate_informal_sentences(self, model,tokenizer, classifier_model, no_repeat_ngram_size)-> None:

        generated_sentences  = []

        samples = len(self.sentence)
        data = self.sentence
        text =  ["paraphrase: " + s + "</s>" for s in data]

        logits_processor_list = LogitsProcessorList([
            ClassifierGuidance(classifier_model, 100, condition_lambda=self.lmbda, condition_class=1),
        ])


        for idx in range(samples):
            input_ids = tokenizer(text[idx], return_tensors="pt", padding=True, truncation=True).input_ids
            result = model.generate(
                    input_ids,
                    max_new_tokens=100,
                    do_sample=False,
                    return_dict_in_generate=True,
                    output_hidden_states=True,
                    output_scores=True,
                    logits_processor=logits_processor_list,
                    no_repeat_ngram_size = no_repeat_ngram_size
                )

            generated_sentences.append(tokenizer.batch_decode(result['sequences'], skip_special_tokens=True)[0])

        self.informal_sentences = generated_sentences
        return

from google.colab import drive
drive.mount('/content/drive')

# load the input sentences
from datasets import load_dataset
import pandas as pd

# enter the path to the file below, which is from dialog_data folder, change it accordingly
file_path = '/content/drive/Shareddrives/COMPSCI685/dialog_data/Dialog_data_input.xlsx'
data = pd.read_excel(file_path)
data = list(data['Sentences'])

from datasets import Dataset
from datasets import load_dataset
from sentence_transformers.losses import CosineSimilarityLoss
from setfit import SetFitModel, SetFitTrainer

# load the models
tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-base", use_fast = True)
model_id = "Vamsi/T5_Paraphrase_Paws"
model = T5ForConditionalGeneration.from_pretrained(model_id)

# enter the path to the model to test below
classifier_model = SetFitModel.from_pretrained('/content/drive/Shareddrives/COMPSCI685/model_checkpoints/gyfac_finetuned')

import time
lmbda = [10,15]
no_repeat_ngram_size = 2
sentence_result = []
for lam in lmbda:
    sentences = Generate_Sentences(data, lam)
    sentences.generate_paraphrase_sentences(model,tokenizer, no_repeat_ngram_size)
    sentences.generate_formal_sentences(model,tokenizer, classifier_model, no_repeat_ngram_size)
    sentences.generate_informal_sentences(model,tokenizer, classifier_model,no_repeat_ngram_size)
    sentence_result.append(sentences)

# convert to csv file
data = [{"sentence": instance.sentence, "lambda": instance.lmbda,
         "formal_sentences": instance.formal_sentences,
         "informal_sentences": instance.informal_sentences}
        for instance in sentence_result]

df = pd.DataFrame(data)
df_exploded = df.explode(["sentence", "formal_sentences", "informal_sentences"]).reset_index(drop=True)


# Pivot the DataFrame
df_pivot = df_exploded.pivot_table(index="sentence",
                          columns="lambda",
                          values=["formal_sentences", "informal_sentences"],
                          aggfunc='first').reset_index()

df_pivot.columns = [f"{col[0]}_lambda_{col[1]}" for col in df_pivot.columns]

# Reset the index
df = df_pivot.rename(columns={"sentence_lambda_": "sentence"})
df['paraphrase'] = sentence_result[0].paraphrase_texts

# enter the path to the result location below
df.to_csv('Result_dialog_data.csv')
